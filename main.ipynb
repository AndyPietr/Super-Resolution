{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iterate over folder with images, count images with the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1404, 2040, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2, os\n",
    "\n",
    "img= cv2.imread('data/DIV2K_train_HR/0001.png')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948f6c23632144eebf3d5699acd9b2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "path = \"data/DIV2K_train_HR\"\n",
    "sizes={}\n",
    "for file in tqdm(os.listdir(\"data/DIV2K_train_HR\")):\n",
    "    image = cv2.imread(path + '/' + file)\n",
    "    if image.shape in sizes.keys():\n",
    "        sizes[image.shape]+=1\n",
    "    else:\n",
    "        sizes[image.shape]=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(648, 2040, 3): 1,\n",
       " (732, 2040, 3): 1,\n",
       " (744, 2040, 3): 1,\n",
       " (768, 2040, 3): 1,\n",
       " (780, 2040, 3): 1,\n",
       " (852, 2040, 3): 1,\n",
       " (864, 2040, 3): 1,\n",
       " (888, 2040, 3): 2,\n",
       " (936, 2040, 3): 1,\n",
       " (948, 2040, 3): 1,\n",
       " (960, 2040, 3): 3,\n",
       " (972, 2040, 3): 1,\n",
       " (1044, 2040, 3): 1,\n",
       " (1068, 2040, 3): 2,\n",
       " (1080, 2040, 3): 1,\n",
       " (1092, 2040, 3): 1,\n",
       " (1128, 2040, 3): 1,\n",
       " (1140, 2040, 3): 9,\n",
       " (1152, 2040, 3): 20,\n",
       " (1164, 2040, 3): 3,\n",
       " (1176, 2040, 3): 2,\n",
       " (1188, 2040, 3): 7,\n",
       " (1200, 2040, 3): 2,\n",
       " (1212, 2040, 3): 4,\n",
       " (1224, 2040, 3): 11,\n",
       " (1248, 2040, 3): 3,\n",
       " (1260, 2040, 3): 8,\n",
       " (1272, 2040, 3): 6,\n",
       " (1284, 2040, 3): 3,\n",
       " (1296, 2040, 3): 6,\n",
       " (1308, 2040, 3): 4,\n",
       " (1320, 2040, 3): 5,\n",
       " (1332, 2040, 3): 10,\n",
       " (1344, 2040, 3): 9,\n",
       " (1356, 2040, 3): 429,\n",
       " (1368, 2040, 3): 28,\n",
       " (1380, 2040, 3): 3,\n",
       " (1392, 2040, 3): 5,\n",
       " (1404, 2040, 3): 1,\n",
       " (1416, 2040, 3): 1,\n",
       " (1428, 2040, 3): 4,\n",
       " (1440, 2040, 3): 3,\n",
       " (1452, 2040, 3): 8,\n",
       " (1464, 2040, 3): 6,\n",
       " (1476, 2040, 3): 1,\n",
       " (1488, 2040, 3): 3,\n",
       " (1512, 2040, 3): 2,\n",
       " (1524, 2040, 3): 3,\n",
       " (1536, 2040, 3): 48,\n",
       " (1548, 2040, 3): 3,\n",
       " (1560, 2040, 3): 1,\n",
       " (1572, 2040, 3): 2,\n",
       " (1584, 2040, 3): 4,\n",
       " (1608, 2040, 3): 1,\n",
       " (1632, 2040, 3): 4,\n",
       " (1644, 2040, 3): 3,\n",
       " (1656, 2040, 3): 1,\n",
       " (1692, 2040, 3): 1,\n",
       " (1728, 2040, 3): 1,\n",
       " (1752, 2040, 3): 1,\n",
       " (1788, 2040, 3): 1,\n",
       " (1800, 2040, 3): 1,\n",
       " (1836, 2040, 3): 1,\n",
       " (1848, 2040, 3): 1,\n",
       " (1992, 2040, 3): 1,\n",
       " (2004, 2040, 3): 1,\n",
       " (2040, 1116, 3): 1,\n",
       " (2040, 1140, 3): 1,\n",
       " (2040, 1164, 3): 2,\n",
       " (2040, 1176, 3): 1,\n",
       " (2040, 1236, 3): 1,\n",
       " (2040, 1248, 3): 1,\n",
       " (2040, 1284, 3): 1,\n",
       " (2040, 1296, 3): 1,\n",
       " (2040, 1320, 3): 1,\n",
       " (2040, 1344, 3): 1,\n",
       " (2040, 1356, 3): 49,\n",
       " (2040, 1368, 3): 5,\n",
       " (2040, 1428, 3): 2,\n",
       " (2040, 1440, 3): 2,\n",
       " (2040, 1476, 3): 1,\n",
       " (2040, 1524, 3): 1,\n",
       " (2040, 1536, 3): 3,\n",
       " (2040, 1572, 3): 1,\n",
       " (2040, 1608, 3): 1,\n",
       " (2040, 1632, 3): 4,\n",
       " (2040, 1644, 3): 1,\n",
       " (2040, 1716, 3): 1,\n",
       " (2040, 1776, 3): 1,\n",
       " (2040, 1824, 3): 1,\n",
       " (2040, 1944, 3): 1,\n",
       " (2040, 1968, 3): 1,\n",
       " (2040, 1992, 3): 1,\n",
       " (2040, 2040, 3): 7}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sorted(sizes.items(), key=lambda item: item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    " There is 429 images of size (1356, 2040, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fiftyone\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"validation\",\n",
    "    label_types=[\"segmentations\"],\n",
    "    classes=[\"cat\", \"dog\"],\n",
    "    max_samples=25,\n",
    ")\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "\n",
    "class ESPCN(nn.Module):\n",
    "    def __init__(self, scale_factor, num_channels=1):\n",
    "        super(ESPCN, self).__init__()\n",
    "        self.first_part = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 64, kernel_size=5, padding=5//2),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=3//2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.last_part = nn.Sequential(\n",
    "            nn.Conv2d(32, num_channels * (scale_factor ** 2), kernel_size=3, padding=3 // 2),\n",
    "            nn.PixelShuffle(scale_factor)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m.in_channels == 32:\n",
    "                    nn.init.normal_(m.weight.data, mean=0.0, std=0.001)\n",
    "                    nn.init.zeros_(m.bias.data)\n",
    "                else:\n",
    "                    nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                    nn.init.zeros_(m.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_part(x)\n",
    "        x = self.last_part(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da2da7cdad0c5b61b0e72159121453c262dd8ac4b6dee56ec366eb5fc2e85a72"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
